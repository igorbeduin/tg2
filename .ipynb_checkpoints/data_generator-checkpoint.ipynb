{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geração do dataset\n",
    "O dataset gerado por este scrip está separado em treino/teste e organizado por classes, pronto para ser utilizado como generator do Tensorflow.\n",
    "Foi criado uma classe Dataset, de onde cada dataset herda uma classe específica. Ex.: o dataset \"covid-chestxray-dataset (<https://github.com/ieee8023/covid-chestxray-dataset>)\" é instanciado pela classe *cohen()*.\n",
    "\n",
    "O pipeline de processamento é:<br>\n",
    "1 - import das bibliotecas<br>\n",
    "2 - instanciação dos objetos de cada dataset<br>\n",
    "3 - Rotina de leitura de cada dataset, aplicando as devidas funções<br>\n",
    "Obs.: nem todos os datasets possuem funções de prefiltragem/posfiltragem. Essas funções foram definidas conforme o processamento feito em <https://github.com/lindawangg/COVID-Net><br>\n",
    "4 - Junção das tabelas de cada ds em uma só (com excessão do RSNA)<br>\n",
    "5.1a - Filtragem das classes de interesse na tabela de imagens<br>\n",
    "5.1b - Separação de imagens específicas para teste, conforme <https://github.com/lindawangg/COVID-Net><br>\n",
    "5.2a - Filtragem das classes de interesse na tabela do RSNA<br>\n",
    "5.2b - Aplica *split* no dataset RSNA<br>\n",
    "6 - Junta as tabelas de treino e teste<br>\n",
    "7 - Monta o dataset no path destino, copiando as imagens que já estão em formato de leitura e escrevendo as imagens em ```.dcm```<br>\n",
    "<br>\n",
    "Obs.: as estapas 3 e 7 podem demorar consideravelmente devido ao dataset RSNA que possui mais de 15k imagens. A fim de verificar a validade do script recomenda-se rodá-lo sem este dataset.<br>\n",
    "<br>\n",
    "As tables ou tabelas referidas nesse script são listas de dicionário do tipo \\[{\"path\": target_path, \"filename\": filename, \"class\": finding, \"url\": url, \"id\": patientid}\\], onde:<br>\n",
    "- target_path = localização da imagem dentro do diretorio do dataset;<br>\n",
    "- filename = nome do arquivo, presente no dataset;<br>\n",
    "- class = classe de classificação da imagem. Ex.: Normal, COVID-19, etc;<br>\n",
    "- url = URL da imagem, usado para detecção de imagens presentes simultaneamente em dois ou mais datasets. Se não há \"url\" no dataset, o valor None é preenchido na tabela;<br>\n",
    "- patientid = utilizado para busca rapida de alguns paciente. Se não há \"patientid\" no dataset, o nome do arquivo sem extensão é utilizado.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT DA LIBS E FUNÇÕES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random \n",
    "import pydicom as dicom\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import rsna, actualmed, cohen, fig1, sirm\n",
    "from ds_utils import filter_table, split_table, table_info, remove_dupl_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONSTROI OS OBJ DE CADA DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_path = os.path.join(os.getcwd(), \"datasets\")\n",
    "datasets_path = \"/home/beduinigor/datasets\"\n",
    "\n",
    "cohen = cohen(datasets_path)\n",
    "rsna = rsna(datasets_path)\n",
    "actualmed = actualmed(datasets_path)\n",
    "fig1 = fig1(datasets_path)\n",
    "sirm = sirm(datasets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LE CADA DS E CONSTROI SUA TABELA APLICANDO AS FUNCOES NECESSARIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [cohen, actualmed, fig1, sirm, rsna]\n",
    "for ds in datasets:\n",
    "    ds.read()\n",
    "    ds.prefilter()\n",
    "    ds.mount_table()\n",
    "    ds.postfilter()\n",
    "    ds.mount_count_table()\n",
    "    if ds.__name__ is sirm.__name__:\n",
    "        remove_dupl_field(sirm, cohen, \"url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JUNTA AS TABELAS EM UMA SÓ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ds = [cohen, actualmed, fig1, sirm]\n",
    "file_table = []\n",
    "for ds in target_ds:\n",
    "    print(f\"Dataset: {ds.__name__.upper()}\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(f\"Imagens: {ds.count}\")\n",
    "    print(f\"Contagem de cada classe por dataset: {ds.count_table}\\n\")\n",
    "    file_table += ds.table\n",
    "\n",
    "print(f\"Total de imagens: {len(file_table)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTRA A TABELA PARA USO APENAS DAS CLASSES DE INTERESSE\n",
    "Se general_case=\"remove\":<br>\n",
    "    - Se a classe da imagen não está em mapping, a imagem é removida da table<br>\n",
    "Se general_case=\"subst\":<br>\n",
    "    - Se a classe da imagen não está em mapping, a classe da imagem é modificada para o valor em \"std_subst\"<br>\n",
    "        Para este caso é possível passar uma lista ```remove_classes```, assim a classe não presente em mapping será substituída no caso padrão mas será removida da table se estiver presente na lista. <br>\n",
    "        Ex.:<br>\n",
    "        - class = \"blabla\" será atribuída o valor de mapping[\"std_subst\"]<br>\n",
    "        - class = \"todo\" terá a imagem removida da tabela<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping = {\"COVID-19\": \"COVID-19\",\n",
    "           \"COVID-19, ARDS\": \"COVID-19\",\n",
    "           \"Normal\": \"Normal\",\n",
    "           \"Pneumonia\": \"Pneumonia\", # OBS.: Linda ignora essa classe\n",
    "           \"SARS\": \"Pneumonia\",\n",
    "           \"MERS\": \"Pneumonia\",\n",
    "           \"Streptococcus\": \"Pneumonia\",\n",
    "           \"Klebsiella\": \"Pneumonia\",\n",
    "           \"Chlamydophila\": \"Pneumonia\",\n",
    "           \"Legionella\": \"Pneumonia\",\n",
    "           \"Lung Opacity\": \"Pneumonia\",\n",
    "           \"1\": \"Pneumonia\",\n",
    "           \"std_subst\": \"Non-COVID\"}\n",
    "# remove_classes = [\"todo\", \"nan\", \"Unknown\"]\n",
    "\n",
    "filtered_table = filter_table(file_table, mapping, general_case=\"remove\")\n",
    "table_info(filtered_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEPARAÇÃO DE TESTES PARA COHEN, FIG1, ACTUALMED E SIRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patients = {\"Pneumonia\": ['8', '31'],\n",
    "                 \"COVID-19\": ['19', '20', '36', '42', '86', \n",
    "                              '94', '97', '117', '132', \n",
    "                              '138', '144', '150', '163', '169', '174', '175', '179', '190', '191',\n",
    "                              'COVID-00024', 'COVID-00025', 'COVID-00026', 'COVID-00027', 'COVID-00029',\n",
    "                              'COVID-00030', 'COVID-00032', 'COVID-00033', 'COVID-00035', 'COVID-00036',\n",
    "                              'COVID-00037', 'COVID-00038',\n",
    "                              'ANON24', 'ANON45', 'ANON126', 'ANON106', 'ANON67',\n",
    "                              'ANON153', 'ANON135', 'ANON44', 'ANON29', 'ANON201', \n",
    "                              'ANON191', 'ANON234', 'ANON110', 'ANON112', 'ANON73', \n",
    "                              'ANON220', 'ANON189', 'ANON30', 'ANON53', 'ANON46',\n",
    "                              'ANON218', 'ANON240', 'ANON100', 'ANON237', 'ANON158',\n",
    "                              'ANON174', 'ANON19', 'ANON195',\n",
    "                              'COVID-19(119)', 'COVID-19(87)', 'COVID-19(70)', 'COVID-19(94)', \n",
    "                              'COVID-19(215)', 'COVID-19(77)', 'COVID-19(213)', 'COVID-19(81)', \n",
    "                              'COVID-19(216)', 'COVID-19(72)', 'COVID-19(106)', 'COVID-19(131)', \n",
    "                              'COVID-19(107)', 'COVID-19(116)', 'COVID-19(95)', 'COVID-19(214)', \n",
    "                              'COVID-19(129)']}\n",
    "\n",
    "test_table = []\n",
    "train_table = []\n",
    "for row in filtered_table:\n",
    "    if row[\"class\"] in test_patients and row[\"id\"] in test_patients[row[\"class\"]]:\n",
    "        test_table.append(row)\n",
    "    else:\n",
    "        train_table.append(row)\n",
    "    \n",
    "print(f\"\\nTRAIN:\\n-------------------\")\n",
    "table_info(train_table)\n",
    "print(f\"\\nTEST:\\n-------------------\")\n",
    "table_info(test_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTRAGEM DAS CLASSES DE INTERESSE DO RSNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsna_filtered_table = filter_table(rsna.table, mapping, general_case=\"remove\")\n",
    "table_info(rsna_filtered_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEPARAÇÃO DE TESTES RSNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.2\n",
    "rsna_train, rsna_test = split_table(rsna_filtered_table, split)\n",
    "print(f\"\\nTRAIN:\\n-------------------\")\n",
    "table_info(rsna_train)\n",
    "print(f\"\\nTEST:\\n-------------------\")\n",
    "table_info(rsna_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JUNTA AS TABLES DE TESTE E TREINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_utils import remove_duplicated\n",
    "\n",
    "train_table += rsna_train\n",
    "test_table += rsna_test\n",
    "\n",
    "print(f\"\\nTRAIN:\\n-------------------\")\n",
    "table_info(train_table)\n",
    "print(f\"\\nTEST:\\n-------------------\")\n",
    "table_info(test_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MONTA O DATASET COPIANDO OS ARQUIVOS DA TABLE (SEPARADOS POR CLASSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ds_utils import mount_dataset\n",
    "\n",
    "mounted_dataset_path = \"./target_dataset\"\n",
    "if not os.path.isdir(mounted_dataset_path):\n",
    "    os.mkdir(mounted_dataset_path)\n",
    "train_path = os.path.join(mounted_dataset_path, \"train\")\n",
    "test_path = os.path.join(mounted_dataset_path, \"test\")\n",
    "\n",
    "print(f\"\\nTRAIN:\\n-------------------\")\n",
    "mount_dataset(train_path, train_table)\n",
    "print(f\"\\nTEST:\\n-------------------\")\n",
    "mount_dataset(test_path, test_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREINAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mounted_dataset_path = \"/media/benchmark/datasets/covidnet/\"\n",
    "image_size = (240, 240)\n",
    "total_training_images = 12460\n",
    "total_test_images = 3080\n",
    "batch_size = 32\n",
    "train_max_batches = int(total_training_images/batch_size)\n",
    "test_max_batches = int(total_test_images/batch_size)\n",
    "color_mode = \"grayscale\"\n",
    "class_mode = \"categorical\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12460 images belonging to 3 classes.\n",
      "Found 3080 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "\n",
    "train_datagen = train_datagen.flow_from_directory(\n",
    "    os.path.join(mounted_dataset_path, \"train\"),\n",
    "    shuffle=True,\n",
    "    target_size=image_size,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_datagen = test_datagen.flow_from_directory(\n",
    "    os.path.join(mounted_dataset_path, \"test\"),\n",
    "    shuffle=True,\n",
    "    target_size=image_size,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training array: \n",
      "  Batch: 0\n",
      "  Batch: 1\n",
      "  Batch: 2\n",
      "  Batch: 3\n",
      "  Batch: 4\n",
      "  Batch: 5\n",
      "  Batch: 6\n",
      "  Batch: 7\n",
      "  Batch: 8\n",
      "  Batch: 9\n",
      "  Batch: 10\n",
      "  Batch: 11\n",
      "  Batch: 12\n",
      "  Batch: 13\n",
      "  Batch: 14\n",
      "  Batch: 15\n",
      "  Batch: 16\n",
      "  Batch: 17\n",
      "  Batch: 18\n",
      "  Batch: 19\n",
      "  Batch: 20\n",
      "  Batch: 21\n",
      "  Batch: 22\n",
      "  Batch: 23\n",
      "  Batch: 24\n",
      "  Batch: 25\n",
      "  Batch: 26\n",
      "  Batch: 27\n",
      "  Batch: 28\n",
      "  Batch: 29\n",
      "  Batch: 30\n",
      "  Batch: 31\n",
      "  Batch: 32\n",
      "  Batch: 33\n",
      "  Batch: 34\n",
      "  Batch: 35\n",
      "  Batch: 36\n",
      "  Batch: 37\n",
      "  Batch: 38\n",
      "  Batch: 39\n",
      "  Batch: 40\n",
      "  Batch: 41\n",
      "  Batch: 42\n",
      "  Batch: 43\n",
      "  Batch: 44\n",
      "  Batch: 45\n",
      "  Batch: 46\n",
      "  Batch: 47\n",
      "  Batch: 48\n",
      "  Batch: 49\n",
      "  Batch: 50\n",
      "  Batch: 51\n",
      "  Batch: 52\n",
      "  Batch: 53\n",
      "  Batch: 54\n",
      "  Batch: 55\n",
      "  Batch: 56\n",
      "  Batch: 57\n",
      "  Batch: 58\n",
      "  Batch: 59\n",
      "  Batch: 60\n",
      "  Batch: 61\n",
      "  Batch: 62\n",
      "  Batch: 63\n",
      "  Batch: 64\n",
      "  Batch: 65\n",
      "  Batch: 66\n",
      "  Batch: 67\n",
      "  Batch: 68\n",
      "  Batch: 69\n",
      "  Batch: 70\n",
      "  Batch: 71\n",
      "  Batch: 72\n",
      "  Batch: 73\n",
      "  Batch: 74\n",
      "  Batch: 75\n",
      "  Batch: 76\n",
      "  Batch: 77\n",
      "  Batch: 78\n",
      "  Batch: 79\n",
      "  Batch: 80\n",
      "  Batch: 81\n",
      "  Batch: 82\n",
      "  Batch: 83\n",
      "  Batch: 84\n",
      "  Batch: 85\n",
      "  Batch: 86\n",
      "  Batch: 87\n",
      "  Batch: 88\n",
      "  Batch: 89\n",
      "  Batch: 90\n",
      "  Batch: 91\n",
      "  Batch: 92\n",
      "  Batch: 93\n",
      "  Batch: 94\n",
      "  Batch: 95\n",
      "  Batch: 96\n",
      "  Batch: 97\n",
      "  Batch: 98\n",
      "  Batch: 99\n",
      "  Batch: 100\n",
      "  Batch: 101\n",
      "  Batch: 102\n",
      "  Batch: 103\n",
      "  Batch: 104\n",
      "  Batch: 105\n",
      "  Batch: 106\n",
      "  Batch: 107\n",
      "  Batch: 108\n",
      "  Batch: 109\n",
      "  Batch: 110\n",
      "  Batch: 111\n",
      "  Batch: 112\n",
      "  Batch: 113\n",
      "  Batch: 114\n",
      "  Batch: 115\n",
      "  Batch: 116\n",
      "  Batch: 117\n",
      "  Batch: 118\n",
      "  Batch: 119\n",
      "  Batch: 120\n",
      "  Batch: 121\n",
      "  Batch: 122\n",
      "  Batch: 123\n",
      "  Batch: 124\n",
      "  Batch: 125\n",
      "  Batch: 126\n",
      "  Batch: 127\n",
      "  Batch: 128\n",
      "  Batch: 129\n",
      "  Batch: 130\n",
      "  Batch: 131\n",
      "  Batch: 132\n",
      "  Batch: 133\n",
      "  Batch: 134\n",
      "  Batch: 135\n",
      "  Batch: 136\n",
      "  Batch: 137\n",
      "  Batch: 138\n",
      "  Batch: 139\n",
      "  Batch: 140\n",
      "  Batch: 141\n",
      "  Batch: 142\n",
      "  Batch: 143\n",
      "  Batch: 144\n",
      "  Batch: 145\n",
      "  Batch: 146\n",
      "  Batch: 147\n",
      "  Batch: 148\n",
      "  Batch: 149\n",
      "  Batch: 150\n",
      "  Batch: 151\n",
      "  Batch: 152\n",
      "  Batch: 153\n",
      "  Batch: 154\n",
      "  Batch: 155\n",
      "  Batch: 156\n",
      "  Batch: 157\n",
      "  Batch: 158\n",
      "  Batch: 159\n",
      "  Batch: 160\n",
      "  Batch: 161\n",
      "  Batch: 162\n",
      "  Batch: 163\n",
      "  Batch: 164\n",
      "  Batch: 165\n",
      "  Batch: 166\n",
      "  Batch: 167\n",
      "  Batch: 168\n",
      "  Batch: 169\n",
      "  Batch: 170\n",
      "  Batch: 171\n",
      "  Batch: 172\n",
      "  Batch: 173\n",
      "  Batch: 174\n",
      "  Batch: 175\n",
      "  Batch: 176\n",
      "  Batch: 177\n",
      "  Batch: 178\n",
      "  Batch: 179\n",
      "  Batch: 180\n",
      "  Batch: 181\n",
      "  Batch: 182\n",
      "  Batch: 183\n",
      "  Batch: 184\n",
      "  Batch: 185\n",
      "  Batch: 186\n",
      "  Batch: 187\n",
      "  Batch: 188\n",
      "  Batch: 189\n",
      "  Batch: 190\n",
      "  Batch: 191\n",
      "  Batch: 192\n",
      "  Batch: 193\n",
      "  Batch: 194\n",
      "  Batch: 195\n",
      "  Batch: 196\n",
      "  Batch: 197\n",
      "  Batch: 198\n",
      "  Batch: 199\n",
      "  Batch: 200\n",
      "  Batch: 201\n",
      "  Batch: 202\n",
      "  Batch: 203\n",
      "  Batch: 204\n",
      "  Batch: 205\n",
      "  Batch: 206\n",
      "  Batch: 207\n",
      "  Batch: 208\n",
      "  Batch: 209\n",
      "  Batch: 210\n",
      "  Batch: 211\n",
      "  Batch: 212\n",
      "  Batch: 213\n",
      "  Batch: 214\n",
      "  Batch: 215\n",
      "  Batch: 216\n",
      "  Batch: 217\n",
      "  Batch: 218\n",
      "  Batch: 219\n",
      "  Batch: 220\n",
      "  Batch: 221\n",
      "  Batch: 222\n",
      "  Batch: 223\n",
      "  Batch: 224\n",
      "  Batch: 225\n",
      "  Batch: 226\n",
      "  Batch: 227\n",
      "  Batch: 228\n",
      "  Batch: 229\n",
      "  Batch: 230\n",
      "  Batch: 231\n",
      "  Batch: 232\n",
      "  Batch: 233\n",
      "  Batch: 234\n",
      "  Batch: 235\n",
      "  Batch: 236\n",
      "  Batch: 237\n",
      "  Batch: 238\n",
      "  Batch: 239\n",
      "  Batch: 240\n",
      "  Batch: 241\n",
      "  Batch: 242\n",
      "  Batch: 243\n",
      "  Batch: 244\n",
      "  Batch: 245\n",
      "  Batch: 246\n",
      "  Batch: 247\n",
      "  Batch: 248\n",
      "  Batch: 249\n",
      "  Batch: 250\n",
      "  Batch: 251\n",
      "  Batch: 252\n",
      "  Batch: 253\n",
      "  Batch: 254\n",
      "  Batch: 255\n",
      "  Batch: 256\n",
      "  Batch: 257\n",
      "  Batch: 258\n",
      "  Batch: 259\n",
      "  Batch: 260\n",
      "  Batch: 261\n",
      "  Batch: 262\n",
      "  Batch: 263\n",
      "  Batch: 264\n",
      "  Batch: 265\n",
      "  Batch: 266\n",
      "  Batch: 267\n",
      "  Batch: 268\n",
      "  Batch: 269\n",
      "  Batch: 270\n",
      "  Batch: 271\n",
      "  Batch: 272\n",
      "  Batch: 273\n",
      "  Batch: 274\n",
      "  Batch: 275\n",
      "  Batch: 276\n",
      "  Batch: 277\n",
      "  Batch: 278\n",
      "  Batch: 279\n",
      "  Batch: 280\n",
      "  Batch: 281\n",
      "  Batch: 282\n",
      "  Batch: 283\n",
      "  Batch: 284\n",
      "  Batch: 285\n",
      "  Batch: 286\n",
      "  Batch: 287\n",
      "  Batch: 288\n",
      "  Batch: 289\n",
      "  Batch: 290\n",
      "  Batch: 291\n",
      "  Batch: 292\n",
      "  Batch: 293\n",
      "  Batch: 294\n",
      "  Batch: 295\n",
      "  Batch: 296\n",
      "  Batch: 297\n",
      "  Batch: 298\n",
      "  Batch: 299\n",
      "  Batch: 300\n",
      "  Batch: 301\n",
      "  Batch: 302\n",
      "  Batch: 303\n",
      "  Batch: 304\n",
      "  Batch: 305\n",
      "  Batch: 306\n",
      "  Batch: 307\n",
      "  Batch: 308\n",
      "  Batch: 309\n",
      "  Batch: 310\n",
      "  Batch: 311\n",
      "  Batch: 312\n",
      "  Batch: 313\n",
      "  Batch: 314\n",
      "  Batch: 315\n",
      "  Batch: 316\n",
      "  Batch: 317\n",
      "  Batch: 318\n",
      "  Batch: 319\n",
      "  Batch: 320\n",
      "  Batch: 321\n",
      "  Batch: 322\n",
      "  Batch: 323\n",
      "  Batch: 324\n",
      "  Batch: 325\n",
      "  Batch: 326\n",
      "  Batch: 327\n",
      "  Batch: 328\n",
      "  Batch: 329\n",
      "  Batch: 330\n",
      "  Batch: 331\n",
      "  Batch: 332\n",
      "  Batch: 333\n",
      "  Batch: 334\n",
      "  Batch: 335\n",
      "  Batch: 336\n",
      "  Batch: 337\n",
      "  Batch: 338\n",
      "  Batch: 339\n",
      "  Batch: 340\n",
      "  Batch: 341\n",
      "  Batch: 342\n",
      "  Batch: 343\n",
      "  Batch: 344\n",
      "  Batch: 345\n",
      "  Batch: 346\n",
      "  Batch: 347\n",
      "  Batch: 348\n",
      "  Batch: 349\n",
      "  Batch: 350\n",
      "  Batch: 351\n",
      "  Batch: 352\n",
      "  Batch: 353\n",
      "  Batch: 354\n",
      "  Batch: 355\n",
      "  Batch: 356\n",
      "  Batch: 357\n",
      "  Batch: 358\n",
      "  Batch: 359\n",
      "  Batch: 360\n",
      "  Batch: 361\n",
      "  Batch: 362\n",
      "  Batch: 363\n",
      "  Batch: 364\n",
      "  Batch: 365\n",
      "  Batch: 366\n",
      "  Batch: 367\n",
      "  Batch: 368\n",
      "  Batch: 369\n",
      "  Batch: 370\n",
      "  Batch: 371\n",
      "  Batch: 372\n",
      "  Batch: 373\n",
      "  Batch: 374\n",
      "  Batch: 375\n",
      "  Batch: 376\n",
      "  Batch: 377\n",
      "  Batch: 378\n",
      "  Batch: 379\n",
      "  Batch: 380\n",
      "  Batch: 381\n",
      "  Batch: 382\n",
      "  Batch: 383\n",
      "  Batch: 384\n",
      "  Batch: 385\n",
      "  Batch: 386\n",
      "  Batch: 387\n",
      "  Batch: 388\n",
      "\n",
      "Creating test array:\n",
      "  Batch: 0\n",
      "  Batch: 1\n",
      "  Batch: 2\n",
      "  Batch: 3\n",
      "  Batch: 4\n",
      "  Batch: 5\n",
      "  Batch: 6\n",
      "  Batch: 7\n",
      "  Batch: 8\n",
      "  Batch: 9\n",
      "  Batch: 10\n",
      "  Batch: 11\n",
      "  Batch: 12\n",
      "  Batch: 13\n",
      "  Batch: 14\n",
      "  Batch: 15\n",
      "  Batch: 16\n",
      "  Batch: 17\n",
      "  Batch: 18\n",
      "  Batch: 19\n",
      "  Batch: 20\n",
      "  Batch: 21\n",
      "  Batch: 22\n",
      "  Batch: 23\n",
      "  Batch: 24\n",
      "  Batch: 25\n",
      "  Batch: 26\n",
      "  Batch: 27\n",
      "  Batch: 28\n",
      "  Batch: 29\n",
      "  Batch: 30\n",
      "  Batch: 31\n",
      "  Batch: 32\n",
      "  Batch: 33\n",
      "  Batch: 34\n",
      "  Batch: 35\n",
      "  Batch: 36\n",
      "  Batch: 37\n",
      "  Batch: 38\n",
      "  Batch: 39\n",
      "  Batch: 40\n",
      "  Batch: 41\n",
      "  Batch: 42\n",
      "  Batch: 43\n",
      "  Batch: 44\n",
      "  Batch: 45\n",
      "  Batch: 46\n",
      "  Batch: 47\n",
      "  Batch: 48\n",
      "  Batch: 49\n",
      "  Batch: 50\n",
      "  Batch: 51\n",
      "  Batch: 52\n",
      "  Batch: 53\n",
      "  Batch: 54\n",
      "  Batch: 55\n",
      "  Batch: 56\n",
      "  Batch: 57\n",
      "  Batch: 58\n",
      "  Batch: 59\n",
      "  Batch: 60\n",
      "  Batch: 61\n",
      "  Batch: 62\n",
      "  Batch: 63\n",
      "  Batch: 64\n",
      "  Batch: 65\n",
      "  Batch: 66\n",
      "  Batch: 67\n",
      "  Batch: 68\n",
      "  Batch: 69\n",
      "  Batch: 70\n",
      "  Batch: 71\n",
      "  Batch: 72\n",
      "  Batch: 73\n",
      "  Batch: 74\n",
      "  Batch: 75\n",
      "  Batch: 76\n",
      "  Batch: 77\n",
      "  Batch: 78\n",
      "  Batch: 79\n",
      "  Batch: 80\n",
      "  Batch: 81\n",
      "  Batch: 82\n",
      "  Batch: 83\n",
      "  Batch: 84\n",
      "  Batch: 85\n",
      "  Batch: 86\n",
      "  Batch: 87\n",
      "  Batch: 88\n",
      "  Batch: 89\n",
      "  Batch: 90\n",
      "  Batch: 91\n",
      "  Batch: 92\n",
      "  Batch: 93\n",
      "  Batch: 94\n",
      "  Batch: 95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "print(\"Creating training array: \")\n",
    "for i in range(train_max_batches):\n",
    "    print(f\"  Batch: {i}\")\n",
    "    batch = train_datagen.next()\n",
    "    x_train.extend(batch[0])\n",
    "    y_train.extend(batch[1])\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "print(\"\\nCreating test array:\")\n",
    "for i in range(test_max_batches):\n",
    "    print(f\"  Batch: {i}\")\n",
    "    batch = test_datagen.next()\n",
    "    x_test.extend(batch[0])\n",
    "    y_test.extend(batch[1])\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "----------\n",
      "(12448, 240, 240, 1)\n",
      "(12448, 3)\n",
      "\n",
      "TEST\n",
      "----------\n",
      "(3072, 240, 240, 1)\n",
      "(3072, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN\\n----------\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"\\nTEST\\n----------\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tb_log_dir = \"/media/training/covidnet/tensorboard\"\n",
    "#checkpoint_path = \"/media/training/covidnet/checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "\n",
    "tb_log_dir = \"/home/beduinigor/tg2_COVIDNet/tensorboard\"\n",
    "checkpoint_path = \"/home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:{epoch:02d}-acc:{accuracy:.2f}.h5\"\n",
    "checkpoint_epochs = 7000\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    min_delta=0,\n",
    "                    patience=20,\n",
    "                    verbose=1,\n",
    "                    mode=\"auto\",\n",
    "                    baseline=None,\n",
    "                    restore_best_weights=True)\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "                    log_dir=tb_log_dir,\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=False,\n",
    "                    update_freq=\"epoch\",\n",
    "                    profile_batch=100000000000,\n",
    "                    embeddings_freq=0,\n",
    "                    embeddings_metadata=None)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                    checkpoint_path,\n",
    "                    monitor=\"val_loss\",\n",
    "                    verbose=1,\n",
    "                    save_best_only=False,\n",
    "                    save_weights_only=False,\n",
    "                    mode=\"auto\",\n",
    "                    save_freq=checkpoint_epochs,\n",
    "                    options=None)\n",
    "\n",
    "callbacks = [tensorboard, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 27m 00s]\n",
      "val_accuracy: 0.8729707598686218\n",
      "\n",
      "Best val_accuracy So Far: 0.8989447951316833\n",
      "Total elapsed time: 08h 20m 14s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/100\n",
      "389/389 [==============================] - 82s 212ms/step - loss: 1.1991 - accuracy: 0.3932\n",
      "Epoch 2/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 0.8004 - accuracy: 0.6522\n",
      "Epoch 3/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 0.5109 - accuracy: 0.8152\n",
      "Epoch 4/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 0.3840 - accuracy: 0.8601s -\n",
      "Epoch 5/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 0.2883 - accuracy: 0.8961\n",
      "Epoch 6/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 0.1996 - accuracy: 0.9287\n",
      "Epoch 7/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 0.1282 - accuracy: 0.9604s - loss: 0.1285 - accuracy: \n",
      "Epoch 8/100\n",
      "389/389 [==============================] - 83s 215ms/step - loss: 0.0697 - accuracy: 0.9855\n",
      "Epoch 9/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 0.0338 - accuracy: 0.9965\n",
      "Epoch 10/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 0.0162 - accuracy: 0.9994\n",
      "Epoch 11/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 0.0080 - accuracy: 0.9999\n",
      "Epoch 12/100\n",
      "389/389 [==============================] - 84s 217ms/step - loss: 0.0047 - accuracy: 0.9999\n",
      "Epoch 13/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "389/389 [==============================] - 84s 216ms/step - loss: 9.7344e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 7.5032e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "386/389 [============================>.] - ETA: 0s - loss: 5.3319e-04 - accuracy: 1.0000\n",
      "Epoch 00018: saving model to /home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:18-acc:1.00.h5\n",
      "\n",
      "Epoch 00018: saving model to /home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:18-acc:1.00.h5\n",
      "389/389 [==============================] - 117s 302ms/step - loss: 5.3220e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "389/389 [==============================] - 82s 212ms/step - loss: 4.2286e-04 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "389/389 [==============================] - 84s 217ms/step - loss: 3.3015e-04 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 2.5065e-04 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 2.0152e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 1.5859e-04 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "389/389 [==============================] - 84s 217ms/step - loss: 1.2006e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 9.8301e-05 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 7.8420e-05 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "389/389 [==============================] - 83s 215ms/step - loss: 6.3656e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "389/389 [==============================] - 84s 217ms/step - loss: 4.9712e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 4.0652e-05 - accuracy: 1.0000s - loss: 4.0927e-05 \n",
      "Epoch 30/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 3.4172e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "389/389 [==============================] - 84s 216ms/step - loss: 2.6864e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 2.2620e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 1.8293e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "389/389 [==============================] - 83s 215ms/step - loss: 1.4864e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 1.2759e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "384/389 [============================>.] - ETA: 1s - loss: 9.9891e-06 - accuracy: 1.0000\n",
      "Epoch 00036: saving model to /home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:36-acc:1.00.h5\n",
      "\n",
      "Epoch 00036: saving model to /home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:36-acc:1.00.h5\n",
      "389/389 [==============================] - 107s 275ms/step - loss: 9.9767e-06 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "389/389 [==============================] - 83s 212ms/step - loss: 8.3459e-06 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 6.8023e-06 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "389/389 [==============================] - 83s 215ms/step - loss: 5.8524e-06 - accuracy: 1.0000s - loss: 5.8739e-06 \n",
      "Epoch 40/100\n",
      "389/389 [==============================] - 82s 212ms/step - loss: 5.1814e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 4.1749e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 3.5460e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "389/389 [==============================] - 84s 216ms/step - loss: 3.0475e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 2.4596e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 2.2555e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 1.8810e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 1.5714e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 1.3354e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 1.3334e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "389/389 [==============================] - 84s 216ms/step - loss: 1.1834e-06 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 9.8336e-07 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 8.4199e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 7.3920e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "382/389 [============================>.] - ETA: 1s - loss: 7.1520e-07 - accuracy: 1.0000\n",
      "Epoch 00054: saving model to /home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:54-acc:1.00.h5\n",
      "\n",
      "Epoch 00054: saving model to /home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:54-acc:1.00.h5\n",
      "389/389 [==============================] - 110s 282ms/step - loss: 7.0890e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "389/389 [==============================] - 82s 212ms/step - loss: 5.8674e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 5.4901e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 4.6198e-07 - accuracy: 1.0000s - loss: 4.6054e-07 - accuracy: 1.\n",
      "Epoch 58/100\n",
      "389/389 [==============================] - 83s 212ms/step - loss: 4.3286e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 3.9692e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 3.6477e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 3.5282e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "389/389 [==============================] - 84s 216ms/step - loss: 3.0686e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 2.6964e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 2.7236e-07 - accuracy: 1.0000\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389/389 [==============================] - 83s 213ms/step - loss: 2.4990e-07 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 2.4064e-07 - accuracy: 1.0000s - loss: 2.432\n",
      "Epoch 67/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 2.3796e-07 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 2.1113e-07 - accuracy: 1.0000s - loss: 2\n",
      "Epoch 69/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 2.1838e-07 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "389/389 [==============================] - 83s 212ms/step - loss: 1.9773e-07 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 1.8675e-07 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "380/389 [============================>.] - ETA: 1s - loss: 1.8679e-07 - accuracy: 1.0000\n",
      "Epoch 00072: saving model to /home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:72-acc:1.00.h5\n",
      "\n",
      "Epoch 00072: saving model to /home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:72-acc:1.00.h5\n",
      "389/389 [==============================] - 113s 290ms/step - loss: 1.8708e-07 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "389/389 [==============================] - 82s 211ms/step - loss: 1.6239e-07 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "389/389 [==============================] - 82s 211ms/step - loss: 1.6932e-07 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "389/389 [==============================] - 83s 212ms/step - loss: 1.6803e-07 - accuracy: 1.0000s - loss: 1.6\n",
      "Epoch 76/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 1.4006e-07 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 1.4761e-07 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "389/389 [==============================] - 84s 216ms/step - loss: 1.5175e-07 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 1.4022e-07 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "389/389 [==============================] - 83s 212ms/step - loss: 1.3384e-07 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "389/389 [==============================] - 84s 216ms/step - loss: 1.3460e-07 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 1.2363e-07 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 1.2247e-07 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 1.2415e-07 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 1.2428e-07 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "389/389 [==============================] - 84s 216ms/step - loss: 1.1955e-07 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 1.2128e-07 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "389/389 [==============================] - 83s 215ms/step - loss: 1.1878e-07 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 1.1229e-07 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "378/389 [============================>.] - ETA: 2s - loss: 1.1634e-07 - accuracy: 1.0000\n",
      "Epoch 00090: saving model to /home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:90-acc:1.00.h5\n",
      "\n",
      "Epoch 00090: saving model to /home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:90-acc:1.00.h5\n",
      "389/389 [==============================] - 107s 275ms/step - loss: 1.1630e-07 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "389/389 [==============================] - 82s 211ms/step - loss: 1.1495e-07 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 1.1686e-07 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 1.1259e-07 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 1.1481e-07 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 1.1031e-07 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "389/389 [==============================] - 83s 213ms/step - loss: 1.1381e-07 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 1.1866e-07 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 1.1422e-07 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "389/389 [==============================] - 83s 214ms/step - loss: 1.1409e-07 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "389/389 [==============================] - 84s 215ms/step - loss: 1.2236e-07 - accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./auto_model/best_model/assets\n"
     ]
    }
   ],
   "source": [
    "import autokeras as ak\n",
    "\n",
    "input_node = ak.ImageInput()\n",
    "output_node = ak.ImageBlock(\n",
    "    # Only search ResNet architectures.\n",
    "    block_type=\"resnet\",\n",
    "    # Normalize the dataset.\n",
    "    normalize=False,\n",
    "    # Do not do data augmentation.\n",
    "    augment=False,\n",
    ")(input_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "\n",
    "model = ak.AutoModel(\n",
    "    inputs=input_node, \n",
    "    outputs=output_node,\n",
    "    objective=\"val_accuracy\",\n",
    "    overwrite=False,\n",
    "    max_trials=20)\n",
    "\n",
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks, \n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model.export_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save(\"checkpoints/final_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEITURA DE PESOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/96 [..............................] - ETA: 5s - loss: 0.7544 - accuracy: 0.9375WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0243s vs `on_test_batch_end` time: 0.0424s). Check your callbacks.\n",
      "96/96 [==============================] - 6s 60ms/step - loss: 0.8545 - accuracy: 0.9124\n",
      "[0.8544848561286926, 0.9124348759651184]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "checkpoints_path = \"/home/beduinigor/tg2_COVIDNet/checkpoints\"\n",
    "auto_model_path = \"/media/training/covidnet/auto_model\"\n",
    "best_model_path = \"/media/training/covidnet/checkpoints/final_model.h5\"\n",
    "\n",
    "loaded_model = load_model(best_model_path)\n",
    "print(loaded_model.evaluate(x_test, y_test, batch_size=batch_size))\n",
    "#for cp in os.listdir(checkpoints_path):\n",
    "#    loaded_model = load_model(os.path.join(checkpoints_path, cp))\n",
    "#    print(loaded_model.evaluate(x_test, y_test, batch_size=batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covidnet_igor",
   "language": "python",
   "name": "covidnet_igor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
