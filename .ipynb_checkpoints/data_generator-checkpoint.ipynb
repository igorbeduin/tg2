{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3QMmr_GtRa7"
   },
   "source": [
    "# Geração do dataset\n",
    "O dataset gerado por este scrip está separado em treino/teste e organizado por classes, pronto para ser utilizado como generator do Tensorflow.\n",
    "Foi criado uma classe Dataset, de onde cada dataset herda uma classe específica. Ex.: o dataset \"covid-chestxray-dataset (<https://github.com/ieee8023/covid-chestxray-dataset>)\" é instanciado pela classe *cohen()*.\n",
    "\n",
    "O pipeline de processamento é:<br>\n",
    "1 - import das bibliotecas<br>\n",
    "2 - instanciação dos objetos de cada dataset<br>\n",
    "3 - Rotina de leitura de cada dataset, aplicando as devidas funções<br>\n",
    "Obs.: nem todos os datasets possuem funções de prefiltragem/posfiltragem. Essas funções foram definidas conforme o processamento feito em <https://github.com/lindawangg/COVID-Net><br>\n",
    "4 - Junção das tabelas de cada ds em uma só (com excessão do RSNA)<br>\n",
    "5.1a - Filtragem das classes de interesse na tabela de imagens<br>\n",
    "5.1b - Separação de imagens específicas para teste, conforme <https://github.com/lindawangg/COVID-Net><br>\n",
    "5.2a - Filtragem das classes de interesse na tabela do RSNA<br>\n",
    "5.2b - Aplica *split* no dataset RSNA<br>\n",
    "6 - Junta as tabelas de treino e teste<br>\n",
    "7 - Monta o dataset no path destino, copiando as imagens que já estão em formato de leitura e escrevendo as imagens em ```.dcm```<br>\n",
    "<br>\n",
    "Obs.: as estapas 3 e 7 podem demorar consideravelmente devido ao dataset RSNA que possui mais de 15k imagens. A fim de verificar a validade do script recomenda-se rodá-lo sem este dataset.<br>\n",
    "<br>\n",
    "As tables ou tabelas referidas nesse script são listas de dicionário do tipo \\[{\"path\": target_path, \"filename\": filename, \"class\": finding, \"url\": url, \"id\": patientid}\\], onde:<br>\n",
    "- target_path = localização da imagem dentro do diretorio do dataset;<br>\n",
    "- filename = nome do arquivo, presente no dataset;<br>\n",
    "- class = classe de classificação da imagem. Ex.: Normal, COVID-19, etc;<br>\n",
    "- url = URL da imagem, usado para detecção de imagens presentes simultaneamente em dois ou mais datasets. Se não há \"url\" no dataset, o valor None é preenchido na tabela;<br>\n",
    "- patientid = utilizado para busca rapida de alguns paciente. Se não há \"patientid\" no dataset, o nome do arquivo sem extensão é utilizado.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26942,
     "status": "ok",
     "timestamp": 1611717811762,
     "user": {
      "displayName": "Igor Beduin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicrLX4A8XIurRrs5tUPFTs_mZ6LzjddQeBQkKTng=s64",
      "userId": "12542818864135131012"
     },
     "user_tz": 180
    },
    "id": "3YI85KgPtjoj",
    "outputId": "0427ee43-7edc-406e-8988-5ad5fca223f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 137798,
     "status": "ok",
     "timestamp": 1611718178618,
     "user": {
      "displayName": "Igor Beduin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicrLX4A8XIurRrs5tUPFTs_mZ6LzjddQeBQkKTng=s64",
      "userId": "12542818864135131012"
     },
     "user_tz": 180
    },
    "id": "MBXPXP-st2Mo",
    "outputId": "5c76af88-a6d5-4ecd-803e-fee94d402755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==0.11.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/58/0aa6fb779dc69cfc811df3398fcbeaeefbf18561b6e36b185df0782781cc/absl_py-0.11.0-py3-none-any.whl (127kB)\n",
      "\r",
      "\u001b[K     |██▋                             | 10kB 27.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 20kB 32.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 30kB 21.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 40kB 20.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 51kB 18.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 61kB 14.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 71kB 13.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 81kB 13.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 92kB 13.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 102kB 13.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 112kB 13.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 122kB 13.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 133kB 13.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: argon2-cffi==20.1.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 2)) (20.1.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 3)) (1.6.3)\n",
      "Requirement already satisfied: async-generator==1.10 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 4)) (1.10)\n",
      "Requirement already satisfied: attrs==20.3.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 5)) (20.3.0)\n",
      "Collecting autokeras==1.0.12\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/12/cf698586ccc8245f08d1843dcafb65b064a2e9e2923b889dc58e1019f099/autokeras-1.0.12-py3-none-any.whl (164kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 26.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: bleach==3.2.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 8)) (3.2.1)\n",
      "Collecting cachetools==4.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\n",
      "Collecting certifi==2020.11.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/6f/3d85f0850962279a7e4c622695d7b3171e95ac65308a57d3b29738b27149/certifi-2020.11.8-py2.py3-none-any.whl (155kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 54.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: cffi==1.14.4 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 11)) (1.14.4)\n",
      "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 12)) (3.0.4)\n",
      "Collecting colorama==0.4.4\n",
      "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 14)) (4.4.2)\n",
      "Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 15)) (0.6.0)\n",
      "Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 16)) (0.3)\n",
      "Collecting future==0.18.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 60.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 18)) (0.3.3)\n",
      "Collecting google-auth==1.23.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/60/81e68e70eea91ef05bb00bcdac243d67b61f826c65aaca6961de622dffd7/google_auth-1.23.0-py2.py3-none-any.whl (114kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 52.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.2 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 21)) (0.4.2)\n",
      "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 22)) (0.2.0)\n",
      "Collecting grpcio==1.34.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/ab/8a7b37278fb59f3688af01cd069a5a4d2f3383eea2a2f78ddea4c7be047a/grpcio-1.34.0-cp36-cp36m-manylinux2014_x86_64.whl (4.0MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0MB 57.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 24)) (2.10.0)\n",
      "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 25)) (2.10)\n",
      "Collecting importlib-metadata==3.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/99/c7/4ccf2baa455613aa9e61372365aba8594ff2806c82189c31a6c65e7c679e/importlib_metadata-3.1.1-py3-none-any.whl\n",
      "Collecting ipykernel==5.3.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/19/c2812690d8b340987eecd2cbc18549b1d130b94c5d97fcbe49f5f8710edf/ipykernel-5.3.4-py3-none-any.whl (120kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 65.0MB/s \n",
      "\u001b[?25hCollecting ipython==7.16.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/6a/210816c943c9aeeb29e4e18a298f14bf0e118fe222a23e13bfcc2d41b0a4/ipython-7.16.1-py3-none-any.whl (785kB)\n",
      "\u001b[K     |████████████████████████████████| 788kB 53.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 29)) (0.2.0)\n",
      "Collecting ipywidgets==7.5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a0/dbcf5881bb2f51e8db678211907f16ea0a182b232c591a6d6f276985ca95/ipywidgets-7.5.1-py2.py3-none-any.whl (121kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 52.8MB/s \n",
      "\u001b[?25hCollecting jedi==0.17.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/d4/36136b18daae06ad798966735f6c3fb96869c1be9f8245d2a8f556e40c36/jedi-0.17.2-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 62.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: Jinja2==2.11.2 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 32)) (2.11.2)\n",
      "Collecting joblib==0.17.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/c9/f58220ac44a1592f79a343caba12f6837f9e0c04c196176a3d66338e1ea8/joblib-0.17.0-py3-none-any.whl (301kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 59.7MB/s \n",
      "\u001b[?25hCollecting jsonschema==3.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 11.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 35)) (1.0.0)\n",
      "Collecting jupyter-client==6.1.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/41/9fa443d5ae8907dd8f7d12146cb0092dc053afd67b5b57e7e8786a328547/jupyter_client-6.1.7-py3-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 66.1MB/s \n",
      "\u001b[?25hCollecting jupyter-console==6.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/65/de/8f9491c4b7e660a75a4eb54694292d918d2d29321936d26bad28cc0530a4/jupyter_console-6.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: jupyter-core==4.7.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 38)) (4.7.0)\n",
      "Requirement already satisfied: jupyterlab-pygments==0.1.2 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 39)) (0.1.2)\n",
      "Requirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 40)) (1.1.2)\n",
      "Collecting keras-tuner==1.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 12.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: Markdown==3.3.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 42)) (3.3.3)\n",
      "Requirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 43)) (1.1.1)\n",
      "Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 44)) (0.8.4)\n",
      "Requirement already satisfied: nbclient==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 45)) (0.5.1)\n",
      "Collecting nbconvert==6.0.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/2f/acbe7006548f3914456ee47f97a2033b1b2f3daf921b12ac94105d87c163/nbconvert-6.0.7-py3-none-any.whl (552kB)\n",
      "\u001b[K     |████████████████████████████████| 552kB 47.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: nbformat==5.0.8 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 47)) (5.0.8)\n",
      "Requirement already satisfied: nest-asyncio==1.4.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 48)) (1.4.3)\n",
      "Collecting notebook==6.1.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/e3/f92ab5688f53dc58dd2cc1b330828024527bb4e8d29bf980a096909b66e5/notebook-6.1.5-py3-none-any.whl (9.5MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5MB 46.2MB/s \n",
      "\u001b[?25hCollecting numpy==1.18.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/b1bc4c935ed063766bce7d3e8c7b20bd52e515ff1c732b02caacf7918e5a/numpy-1.18.5-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1MB 1.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 51)) (3.1.0)\n",
      "Collecting opencv-python==4.4.0.46\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/80/10a9ae6fa0940f25af32739d1dc6dfdbbdc79af3f04c5ea1a6de4303cd54/opencv_python-4.4.0.46-cp36-cp36m-manylinux2014_x86_64.whl (49.5MB)\n",
      "\u001b[K     |████████████████████████████████| 49.5MB 60kB/s \n",
      "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 53)) (3.3.0)\n",
      "Collecting packaging==20.7\n",
      "  Downloading https://files.pythonhosted.org/packages/28/87/8edcf555adaf60d053ead881bc056079e29319b643ca710339ce84413136/packaging-20.7-py2.py3-none-any.whl\n",
      "Collecting pandas==1.1.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/51/bafcff417cd857bc6684336320863b5e5af280530213ef8f534b6042cfe6/pandas-1.1.4-cp36-cp36m-manylinux1_x86_64.whl (9.5MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5MB 45.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandocfilters==1.4.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 56)) (1.4.3)\n",
      "Collecting parso==0.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/d1/e635bdde32890db5aeb2ffbde17e74f68986305a4466b0aa373b861e3f00/parso-0.7.1-py2.py3-none-any.whl (109kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 56.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 58)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 59)) (0.7.5)\n",
      "Collecting Pillow==8.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/19/d4c25111d36163698396f93c363114cf1cddbacb24744f6612f25b6aa3d0/Pillow-8.0.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 46.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: prometheus-client==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 61)) (0.9.0)\n",
      "Collecting prompt-toolkit==3.0.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/aa/198e6a857e83ea8b711a6ae0c37717c0eb1b23ff52e3732a644fcd389cb3/prompt_toolkit-3.0.8-py3-none-any.whl (355kB)\n",
      "\u001b[K     |████████████████████████████████| 358kB 54.7MB/s \n",
      "\u001b[?25hCollecting protobuf==3.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/fd/247ef25f5ec5f9acecfbc98ca3c6aaf66716cf52509aca9a93583d410493/protobuf-3.14.0-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 47.7MB/s \n",
      "\u001b[?25hCollecting ptyprocess==0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 65)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 66)) (0.2.8)\n",
      "Requirement already satisfied: pycparser==2.20 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 67)) (2.20)\n",
      "Collecting pydicom==2.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/7b/6ed88f82dd33a32cdb43432dab7f84fcd40c49d63251442b3cfe0be983d4/pydicom-2.1.1-py3-none-any.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 55.6MB/s \n",
      "\u001b[?25hCollecting Pygments==2.7.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/cf/d7e48846e10ac4f8b89d489cec3dc9c2804a825f812c851559cfbcc251cc/Pygments-2.7.2-py3-none-any.whl (948kB)\n",
      "\u001b[K     |████████████████████████████████| 952kB 53.0MB/s \n",
      "\u001b[?25hCollecting pylibjpeg==1.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/22/f5/a5bbb6c98b52b2188bb3a3919ece158d329aa2a476ec34536bba9e7e88bc/pylibjpeg-1.1.1-py3-none-any.whl\n",
      "Collecting pylibjpeg-openjpeg==1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/ae/21ce161073aba668575c2b53963091d43ccc12a13a411d693b751ffe2aaf/pylibjpeg_openjpeg-1.0.1-cp36-cp36m-manylinux2010_x86_64.whl (3.6MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6MB 55.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 72)) (2.4.7)\n",
      "Requirement already satisfied: pyrsistent==0.17.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 73)) (0.17.3)\n",
      "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 74)) (2.8.1)\n",
      "Collecting pytz==2020.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/f8/ff09af6ff61a3efaad5f61ba5facdf17e7722c4393f7d8a66674d2dbd29f/pytz-2020.4-py2.py3-none-any.whl (509kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 55.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyzmq==20.0.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 76)) (20.0.0)\n",
      "Requirement already satisfied: qtconsole==5.0.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 77)) (5.0.1)\n",
      "Requirement already satisfied: QtPy==1.9.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 78)) (1.9.0)\n",
      "Collecting requests==2.25.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/fc/f91eac5a39a65f75a7adb58eac7fa78871ea9872283fb9c44e6545998134/requests-2.25.0-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 10.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 80)) (1.3.0)\n",
      "Requirement already satisfied: rsa==4.6 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 81)) (4.6)\n",
      "Collecting scikit-learn==0.23.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8MB 50.7MB/s \n",
      "\u001b[?25hCollecting scipy==1.5.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/89/63171228d5ced148f5ced50305c89e8576ffc695a90b58fe5bb602b910c2/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9MB 94kB/s \n",
      "\u001b[?25hRequirement already satisfied: Send2Trash==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 84)) (1.5.0)\n",
      "Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 85)) (1.15.0)\n",
      "Requirement already satisfied: tabulate==0.8.7 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 86)) (0.8.7)\n",
      "Requirement already satisfied: tensorboard==2.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 87)) (2.4.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 88)) (1.7.0)\n",
      "Collecting tensorflow==2.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/ad/769c195c72ac72040635c66cd9ba7b0f4b4fc1ac67e59b99fa6988446c22/tensorflow-2.3.1-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
      "\u001b[K     |████████████████████████████████| 320.4MB 54kB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator==2.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 48.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 91)) (1.1.0)\n",
      "Collecting terminado==0.9.1\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/17/fa9560738187cdb185d282a120ec4a147064f001d7c0114271ca1374d0a1/terminado-0.9.1-py3-none-any.whl\n",
      "Collecting terminaltables==3.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
      "Requirement already satisfied: testpath==0.4.4 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 94)) (0.4.4)\n",
      "Collecting threadpoolctl==2.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Collecting tornado==6.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/26/e710295dcb4aac62b08f22d07efc899574476db37532159a7f71713cdaf2/tornado-6.1-cp36-cp36m-manylinux2010_x86_64.whl (427kB)\n",
      "\u001b[K     |████████████████████████████████| 430kB 36.2MB/s \n",
      "\u001b[?25hCollecting tqdm==4.54.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/4e/afa45872365fe2abd13c8022d39348c01808b8cfeea129937920d7bb2244/tqdm-4.54.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: traitlets==4.3.3 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 98)) (4.3.3)\n",
      "Collecting urllib3==1.26.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl (136kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 58.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: wcwidth==0.2.5 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 100)) (0.2.5)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 101)) (0.5.1)\n",
      "Requirement already satisfied: Werkzeug==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 102)) (1.0.1)\n",
      "Requirement already satisfied: widgetsnbextension==3.5.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 103)) (3.5.1)\n",
      "Requirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 104)) (1.12.1)\n",
      "Collecting xlrd==1.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/16/63576a1a001752e34bf8ea62e367997530dc553b689356b9879339cf45a4/xlrd-1.2.0-py2.py3-none-any.whl (103kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 56.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: zipp==3.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 106)) (3.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse==1.6.3->-r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 3)) (0.36.2)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth==1.23.0->-r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt (line 20)) (51.3.3)\n",
      "Building wheels for collected packages: future, keras-tuner, terminaltables\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=2e8d4fa9b29f5638390265860a67e383982c69ba68f7ec8d565af42209a9ecba\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp36-none-any.whl size=78939 sha256=5573c2820a31b28a9d0047f29c352ec3cd80118ab73dbdb661f90c3286829547\n",
      "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
      "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15358 sha256=325e57c5cc891587f20df9d608ce8f3ff5fd04826848d6dfad45a2f0fec37cac\n",
      "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
      "Successfully built future keras-tuner terminaltables\n",
      "\u001b[31mERROR: tensorflow-metadata 0.26.0 has requirement absl-py<0.11,>=0.9, but you'll have absl-py 0.11.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.17.2, but you'll have google-auth 1.23.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.3.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.16.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement notebook~=5.3.0; python_version >= \"3.0\", but you'll have notebook 6.1.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 6.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: convertdate 2.2.0 has requirement pytz<2020,>=2014.10, but you'll have pytz 2020.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: absl-py, packaging, tensorflow-estimator, protobuf, numpy, grpcio, tensorflow, future, terminaltables, colorama, tqdm, certifi, urllib3, requests, scipy, joblib, threadpoolctl, scikit-learn, keras-tuner, pytz, pandas, autokeras, cachetools, google-auth, importlib-metadata, tornado, prompt-toolkit, parso, jedi, Pygments, ipython, jupyter-client, ipykernel, ipywidgets, jsonschema, jupyter-console, nbconvert, ptyprocess, terminado, notebook, opencv-python, Pillow, pydicom, pylibjpeg-openjpeg, pylibjpeg, xlrd\n",
      "  Found existing installation: absl-py 0.10.0\n",
      "    Uninstalling absl-py-0.10.0:\n",
      "      Successfully uninstalled absl-py-0.10.0\n",
      "  Found existing installation: packaging 20.8\n",
      "    Uninstalling packaging-20.8:\n",
      "      Successfully uninstalled packaging-20.8\n",
      "  Found existing installation: tensorflow-estimator 2.4.0\n",
      "    Uninstalling tensorflow-estimator-2.4.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
      "  Found existing installation: protobuf 3.12.4\n",
      "    Uninstalling protobuf-3.12.4:\n",
      "      Successfully uninstalled protobuf-3.12.4\n",
      "  Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Found existing installation: grpcio 1.32.0\n",
      "    Uninstalling grpcio-1.32.0:\n",
      "      Successfully uninstalled grpcio-1.32.0\n",
      "  Found existing installation: tensorflow 2.4.0\n",
      "    Uninstalling tensorflow-2.4.0:\n",
      "      Successfully uninstalled tensorflow-2.4.0\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "  Found existing installation: certifi 2020.12.5\n",
      "    Uninstalling certifi-2020.12.5:\n",
      "      Successfully uninstalled certifi-2020.12.5\n",
      "  Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "  Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "  Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Found existing installation: joblib 1.0.0\n",
      "    Uninstalling joblib-1.0.0:\n",
      "      Successfully uninstalled joblib-1.0.0\n",
      "  Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "  Found existing installation: pytz 2018.9\n",
      "    Uninstalling pytz-2018.9:\n",
      "      Successfully uninstalled pytz-2018.9\n",
      "  Found existing installation: pandas 1.1.5\n",
      "    Uninstalling pandas-1.1.5:\n",
      "      Successfully uninstalled pandas-1.1.5\n",
      "  Found existing installation: cachetools 4.2.0\n",
      "    Uninstalling cachetools-4.2.0:\n",
      "      Successfully uninstalled cachetools-4.2.0\n",
      "  Found existing installation: google-auth 1.17.2\n",
      "    Uninstalling google-auth-1.17.2:\n",
      "      Successfully uninstalled google-auth-1.17.2\n",
      "  Found existing installation: importlib-metadata 3.3.0\n",
      "    Uninstalling importlib-metadata-3.3.0:\n",
      "      Successfully uninstalled importlib-metadata-3.3.0\n",
      "  Found existing installation: tornado 5.1.1\n",
      "    Uninstalling tornado-5.1.1:\n",
      "      Successfully uninstalled tornado-5.1.1\n",
      "  Found existing installation: prompt-toolkit 1.0.18\n",
      "    Uninstalling prompt-toolkit-1.0.18:\n",
      "      Successfully uninstalled prompt-toolkit-1.0.18\n",
      "  Found existing installation: parso 0.8.1\n",
      "    Uninstalling parso-0.8.1:\n",
      "      Successfully uninstalled parso-0.8.1\n",
      "  Found existing installation: jedi 0.18.0\n",
      "    Uninstalling jedi-0.18.0:\n",
      "      Successfully uninstalled jedi-0.18.0\n",
      "  Found existing installation: Pygments 2.6.1\n",
      "    Uninstalling Pygments-2.6.1:\n",
      "      Successfully uninstalled Pygments-2.6.1\n",
      "  Found existing installation: ipython 5.5.0\n",
      "    Uninstalling ipython-5.5.0:\n",
      "      Successfully uninstalled ipython-5.5.0\n",
      "  Found existing installation: jupyter-client 5.3.5\n",
      "    Uninstalling jupyter-client-5.3.5:\n",
      "      Successfully uninstalled jupyter-client-5.3.5\n",
      "  Found existing installation: ipykernel 4.10.1\n",
      "    Uninstalling ipykernel-4.10.1:\n",
      "      Successfully uninstalled ipykernel-4.10.1\n",
      "  Found existing installation: ipywidgets 7.6.3\n",
      "    Uninstalling ipywidgets-7.6.3:\n",
      "      Successfully uninstalled ipywidgets-7.6.3\n",
      "  Found existing installation: jsonschema 2.6.0\n",
      "    Uninstalling jsonschema-2.6.0:\n",
      "      Successfully uninstalled jsonschema-2.6.0\n",
      "  Found existing installation: jupyter-console 5.2.0\n",
      "    Uninstalling jupyter-console-5.2.0:\n",
      "      Successfully uninstalled jupyter-console-5.2.0\n",
      "  Found existing installation: nbconvert 5.6.1\n",
      "    Uninstalling nbconvert-5.6.1:\n",
      "      Successfully uninstalled nbconvert-5.6.1\n",
      "  Found existing installation: ptyprocess 0.7.0\n",
      "    Uninstalling ptyprocess-0.7.0:\n",
      "      Successfully uninstalled ptyprocess-0.7.0\n",
      "  Found existing installation: terminado 0.9.2\n",
      "    Uninstalling terminado-0.9.2:\n",
      "      Successfully uninstalled terminado-0.9.2\n",
      "  Found existing installation: notebook 5.3.1\n",
      "    Uninstalling notebook-5.3.1:\n",
      "      Successfully uninstalled notebook-5.3.1\n",
      "  Found existing installation: opencv-python 4.1.2.30\n",
      "    Uninstalling opencv-python-4.1.2.30:\n",
      "      Successfully uninstalled opencv-python-4.1.2.30\n",
      "  Found existing installation: Pillow 7.0.0\n",
      "    Uninstalling Pillow-7.0.0:\n",
      "      Successfully uninstalled Pillow-7.0.0\n",
      "  Found existing installation: xlrd 1.1.0\n",
      "    Uninstalling xlrd-1.1.0:\n",
      "      Successfully uninstalled xlrd-1.1.0\n",
      "Successfully installed Pillow-8.0.1 Pygments-2.7.2 absl-py-0.11.0 autokeras-1.0.12 cachetools-4.1.1 certifi-2020.11.8 colorama-0.4.4 future-0.18.2 google-auth-1.23.0 grpcio-1.34.0 importlib-metadata-3.1.1 ipykernel-5.3.4 ipython-7.16.1 ipywidgets-7.5.1 jedi-0.17.2 joblib-0.17.0 jsonschema-3.2.0 jupyter-client-6.1.7 jupyter-console-6.2.0 keras-tuner-1.0.2 nbconvert-6.0.7 notebook-6.1.5 numpy-1.18.5 opencv-python-4.4.0.46 packaging-20.7 pandas-1.1.4 parso-0.7.1 prompt-toolkit-3.0.8 protobuf-3.14.0 ptyprocess-0.6.0 pydicom-2.1.1 pylibjpeg-1.1.1 pylibjpeg-openjpeg-1.0.1 pytz-2020.4 requests-2.25.0 scikit-learn-0.23.2 scipy-1.5.4 tensorflow-2.3.1 tensorflow-estimator-2.3.0 terminado-0.9.1 terminaltables-3.1.0 threadpoolctl-2.1.0 tornado-6.1 tqdm-4.54.0 urllib3-1.26.2 xlrd-1.2.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "IPython",
         "PIL",
         "absl",
         "certifi",
         "google",
         "ipykernel",
         "ipywidgets",
         "jupyter_client",
         "numpy",
         "pandas",
         "prompt_toolkit",
         "pygments",
         "pytz",
         "requests",
         "scipy",
         "tensorflow",
         "tornado",
         "urllib3"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# installl requirements on google colab\n",
    "!pip install -r drive/MyDrive/TG/tg2_COVIDNet/requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcfo9r7atRbN"
   },
   "source": [
    "## IMPORT DA LIBS E FUNÇÕES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txkoq6GitRbP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random \n",
    "import pydicom as dicom\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFIlPNNWyoQP"
   },
   "outputs": [],
   "source": [
    "os.chdir('drive/MyDrive/TG/tg2_COVIDNet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cx2oQsxStRbR"
   },
   "outputs": [],
   "source": [
    "from datasets import rsna, actualmed, cohen, fig1, sirm\n",
    "from ds_utils import filter_table, split_table, table_info, remove_dupl_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPeVZeGCtRbT"
   },
   "source": [
    "## CONSTROI OS OBJ DE CADA DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9v98wzeMFTo"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-M_d4GzqtRbU"
   },
   "outputs": [],
   "source": [
    "# datasets_path = os.path.join(os.getcwd(), \"datasets\")\n",
    "datasets_path = \"/Users/igorbeduin/Google Drive (beduinigor@gmail.com)/TG/datasets\"\n",
    "\n",
    "cohen = cohen(datasets_path)\n",
    "rsna = rsna(datasets_path)\n",
    "actualmed = actualmed(datasets_path)\n",
    "fig1 = fig1(datasets_path)\n",
    "sirm = sirm(datasets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzDtxm8KtRbU"
   },
   "source": [
    "## LE CADA DS E CONSTROI SUA TABELA APLICANDO AS FUNCOES NECESSARIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3YG1D9o6tRbV"
   },
   "outputs": [],
   "source": [
    "datasets = [cohen, actualmed, fig1, sirm, rsna]\n",
    "for ds in datasets:\n",
    "    ds.read()\n",
    "    ds.prefilter()\n",
    "    ds.mount_table()\n",
    "    ds.postfilter()\n",
    "    ds.mount_count_table()\n",
    "    if ds.__name__ is sirm.__name__:\n",
    "        remove_dupl_field(sirm, cohen, \"url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UWyaQaDtRbV"
   },
   "source": [
    "## JUNTA AS TABELAS EM UMA SÓ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GJvksodtRbX"
   },
   "outputs": [],
   "source": [
    "target_ds = [cohen, actualmed, fig1, sirm]\n",
    "file_table = []\n",
    "for ds in target_ds:\n",
    "    print(f\"Dataset: {ds.__name__.upper()}\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(f\"Imagens: {ds.count}\")\n",
    "    print(f\"Contagem de cada classe por dataset: {ds.count_table}\\n\")\n",
    "    file_table += ds.table\n",
    "\n",
    "print(f\"Total de imagens: {len(file_table)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vm91D7QJtRbX"
   },
   "source": [
    "## FILTRA A TABELA PARA USO APENAS DAS CLASSES DE INTERESSE\n",
    "Se general_case=\"remove\":<br>\n",
    "    - Se a classe da imagen não está em mapping, a imagem é removida da table<br>\n",
    "Se general_case=\"subst\":<br>\n",
    "    - Se a classe da imagen não está em mapping, a classe da imagem é modificada para o valor em \"std_subst\"<br>\n",
    "        Para este caso é possível passar uma lista ```remove_classes```, assim a classe não presente em mapping será substituída no caso padrão mas será removida da table se estiver presente na lista. <br>\n",
    "        Ex.:<br>\n",
    "        - class = \"blabla\" será atribuída o valor de mapping[\"std_subst\"]<br>\n",
    "        - class = \"todo\" terá a imagem removida da tabela<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDep0J75tRbY"
   },
   "outputs": [],
   "source": [
    "mapping = {\"COVID-19\": \"COVID-19\",\n",
    "           \"COVID-19, ARDS\": \"COVID-19\",\n",
    "           \"Normal\": \"Normal\",\n",
    "           \"Pneumonia\": \"Pneumonia\", # OBS.: Linda ignora essa classe\n",
    "           \"SARS\": \"Pneumonia\",\n",
    "           \"MERS\": \"Pneumonia\",\n",
    "           \"Streptococcus\": \"Pneumonia\",\n",
    "           \"Klebsiella\": \"Pneumonia\",\n",
    "           \"Chlamydophila\": \"Pneumonia\",\n",
    "           \"Legionella\": \"Pneumonia\",\n",
    "           \"Lung Opacity\": \"Pneumonia\",\n",
    "           \"1\": \"Pneumonia\",\n",
    "           \"std_subst\": \"Non-COVID\"}\n",
    "# remove_classes = [\"todo\", \"nan\", \"Unknown\"]\n",
    "\n",
    "filtered_table = filter_table(file_table, mapping, general_case=\"remove\")\n",
    "table_info(filtered_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXUO0ypjtRbZ"
   },
   "source": [
    "## SEPARAÇÃO DE TESTES PARA COHEN, FIG1, ACTUALMED E SIRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-VwTvIxtRbZ"
   },
   "outputs": [],
   "source": [
    "test_patients = {\"Pneumonia\": ['8', '31'],\n",
    "                 \"COVID-19\": ['19', '20', '36', '42', '86', \n",
    "                              '94', '97', '117', '132', \n",
    "                              '138', '144', '150', '163', '169', '174', '175', '179', '190', '191',\n",
    "                              'COVID-00024', 'COVID-00025', 'COVID-00026', 'COVID-00027', 'COVID-00029',\n",
    "                              'COVID-00030', 'COVID-00032', 'COVID-00033', 'COVID-00035', 'COVID-00036',\n",
    "                              'COVID-00037', 'COVID-00038',\n",
    "                              'ANON24', 'ANON45', 'ANON126', 'ANON106', 'ANON67',\n",
    "                              'ANON153', 'ANON135', 'ANON44', 'ANON29', 'ANON201', \n",
    "                              'ANON191', 'ANON234', 'ANON110', 'ANON112', 'ANON73', \n",
    "                              'ANON220', 'ANON189', 'ANON30', 'ANON53', 'ANON46',\n",
    "                              'ANON218', 'ANON240', 'ANON100', 'ANON237', 'ANON158',\n",
    "                              'ANON174', 'ANON19', 'ANON195',\n",
    "                              'COVID-19(119)', 'COVID-19(87)', 'COVID-19(70)', 'COVID-19(94)', \n",
    "                              'COVID-19(215)', 'COVID-19(77)', 'COVID-19(213)', 'COVID-19(81)', \n",
    "                              'COVID-19(216)', 'COVID-19(72)', 'COVID-19(106)', 'COVID-19(131)', \n",
    "                              'COVID-19(107)', 'COVID-19(116)', 'COVID-19(95)', 'COVID-19(214)', \n",
    "                              'COVID-19(129)']}\n",
    "\n",
    "test_table = []\n",
    "train_table = []\n",
    "for row in filtered_table:\n",
    "    if row[\"class\"] in test_patients and row[\"id\"] in test_patients[row[\"class\"]]:\n",
    "        test_table.append(row)\n",
    "    else:\n",
    "        train_table.append(row)\n",
    "    \n",
    "print(f\"\\nTRAIN:\\n-------------------\")\n",
    "table_info(train_table)\n",
    "print(f\"\\nTEST:\\n-------------------\")\n",
    "table_info(test_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64NCOWBhtRba"
   },
   "source": [
    "## FILTRAGEM DAS CLASSES DE INTERESSE DO RSNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InWMuo_ZtRbb"
   },
   "outputs": [],
   "source": [
    "rsna_filtered_table = filter_table(rsna.table, mapping, general_case=\"remove\")\n",
    "table_info(rsna_filtered_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K71Np5GPtRbb"
   },
   "source": [
    "## SEPARAÇÃO DE TESTES RSNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cknsyRSGtRbc"
   },
   "outputs": [],
   "source": [
    "split = 0.2\n",
    "rsna_train, rsna_test = split_table(rsna_filtered_table, split)\n",
    "print(f\"\\nTRAIN:\\n-------------------\")\n",
    "table_info(rsna_train)\n",
    "print(f\"\\nTEST:\\n-------------------\")\n",
    "table_info(rsna_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf1NECVRtRbk"
   },
   "source": [
    "## JUNTA AS TABLES DE TESTE E TREINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SBRi3h_tRbk"
   },
   "outputs": [],
   "source": [
    "from ds_utils import remove_duplicated\n",
    "\n",
    "train_table += rsna_train\n",
    "test_table += rsna_test\n",
    "\n",
    "print(f\"\\nTRAIN:\\n-------------------\")\n",
    "table_info(train_table)\n",
    "print(f\"\\nTEST:\\n-------------------\")\n",
    "table_info(test_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDgr2SG_tRbl"
   },
   "source": [
    "# MONTA O DATASET COPIANDO OS ARQUIVOS DA TABLE (SEPARADOS POR CLASSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 747,
     "status": "error",
     "timestamp": 1611645664184,
     "user": {
      "displayName": "Igor Beduin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicrLX4A8XIurRrs5tUPFTs_mZ6LzjddQeBQkKTng=s64",
      "userId": "12542818864135131012"
     },
     "user_tz": 180
    },
    "id": "GOkuqCOGtRbl",
    "outputId": "666778ee-05f8-4c87-c2bd-d980e1504072"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-56b0d0b83a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mds_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmount_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmounted_dataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./target_dataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmounted_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmounted_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ds_utils'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ds_utils import mount_dataset\n",
    "\n",
    "mounted_dataset_path = \"./target_dataset\"\n",
    "if not os.path.isdir(mounted_dataset_path):\n",
    "    os.mkdir(mounted_dataset_path)\n",
    "train_path = os.path.join(mounted_dataset_path, \"train\")\n",
    "test_path = os.path.join(mounted_dataset_path, \"test\")\n",
    "\n",
    "print(f\"\\nTRAIN:\\n-------------------\")\n",
    "mount_dataset(train_path, train_table)\n",
    "print(f\"\\nTEST:\\n-------------------\")\n",
    "mount_dataset(test_path, test_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "div7_rpUtRbo"
   },
   "source": [
    "## TREINAMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 788,
     "status": "ok",
     "timestamp": 1611718204740,
     "user": {
      "displayName": "Igor Beduin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicrLX4A8XIurRrs5tUPFTs_mZ6LzjddQeBQkKTng=s64",
      "userId": "12542818864135131012"
     },
     "user_tz": 180
    },
    "id": "Zmh9vQCjtRbp"
   },
   "outputs": [],
   "source": [
    "mounted_dataset_path = '/home/beduinigor/target_dataset'\n",
    "#mounted_dataset_path = \"/content/drive/MyDrive/TG/tg2_COVIDNet/target_dataset\"\n",
    "#mounted_dataset_path = \"./target_dataset\"\n",
    "image_size = (240, 240)\n",
    "total_training_images = 12460\n",
    "total_test_images = 3080\n",
    "batch_size = 32\n",
    "train_max_batches = int(total_training_images/batch_size)\n",
    "test_max_batches = int(total_test_images/batch_size)\n",
    "color_mode = \"grayscale\"\n",
    "class_mode = \"categorical\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tc69CY0QtRbp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12460 images belonging to 3 classes.\n",
      "Found 3080 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True)\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "\n",
    "train_datagen = train_datagen.flow_from_directory(\n",
    "    os.path.join(mounted_dataset_path, \"train\"),\n",
    "    shuffle=True,\n",
    "    target_size=image_size,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_datagen = test_datagen.flow_from_directory(\n",
    "    os.path.join(mounted_dataset_path, \"test\"),\n",
    "    shuffle=True,\n",
    "    target_size=image_size,\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ISaEaxNitRbr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training array: \n",
      "  Batch: 0\n",
      "  Batch: 1\n",
      "  Batch: 2\n",
      "  Batch: 3\n",
      "  Batch: 4\n",
      "  Batch: 5\n",
      "  Batch: 6\n",
      "  Batch: 7\n",
      "  Batch: 8\n",
      "  Batch: 9\n",
      "  Batch: 10\n",
      "  Batch: 11\n",
      "  Batch: 12\n",
      "  Batch: 13\n",
      "  Batch: 14\n",
      "  Batch: 15\n",
      "  Batch: 16\n",
      "  Batch: 17\n",
      "  Batch: 18\n",
      "  Batch: 19\n",
      "  Batch: 20\n",
      "  Batch: 21\n",
      "  Batch: 22\n",
      "  Batch: 23\n",
      "  Batch: 24\n",
      "  Batch: 25\n",
      "  Batch: 26\n",
      "  Batch: 27\n",
      "  Batch: 28\n",
      "  Batch: 29\n",
      "  Batch: 30\n",
      "  Batch: 31\n",
      "  Batch: 32\n",
      "  Batch: 33\n",
      "  Batch: 34\n",
      "  Batch: 35\n",
      "  Batch: 36\n",
      "  Batch: 37\n",
      "  Batch: 38\n",
      "  Batch: 39\n",
      "  Batch: 40\n",
      "  Batch: 41\n",
      "  Batch: 42\n",
      "  Batch: 43\n",
      "  Batch: 44\n",
      "  Batch: 45\n",
      "  Batch: 46\n",
      "  Batch: 47\n",
      "  Batch: 48\n",
      "  Batch: 49\n",
      "  Batch: 50\n",
      "  Batch: 51\n",
      "  Batch: 52\n",
      "  Batch: 53\n",
      "  Batch: 54\n",
      "  Batch: 55\n",
      "  Batch: 56\n",
      "  Batch: 57\n",
      "  Batch: 58\n",
      "  Batch: 59\n",
      "  Batch: 60\n",
      "  Batch: 61\n",
      "  Batch: 62\n",
      "  Batch: 63\n",
      "  Batch: 64\n",
      "  Batch: 65\n",
      "  Batch: 66\n",
      "  Batch: 67\n",
      "  Batch: 68\n",
      "  Batch: 69\n",
      "  Batch: 70\n",
      "  Batch: 71\n",
      "  Batch: 72\n",
      "  Batch: 73\n",
      "  Batch: 74\n",
      "  Batch: 75\n",
      "  Batch: 76\n",
      "  Batch: 77\n",
      "  Batch: 78\n",
      "  Batch: 79\n",
      "  Batch: 80\n",
      "  Batch: 81\n",
      "  Batch: 82\n",
      "  Batch: 83\n",
      "  Batch: 84\n",
      "  Batch: 85\n",
      "  Batch: 86\n",
      "  Batch: 87\n",
      "  Batch: 88\n",
      "  Batch: 89\n",
      "  Batch: 90\n",
      "  Batch: 91\n",
      "  Batch: 92\n",
      "  Batch: 93\n",
      "  Batch: 94\n",
      "  Batch: 95\n",
      "  Batch: 96\n",
      "  Batch: 97\n",
      "  Batch: 98\n",
      "  Batch: 99\n",
      "  Batch: 100\n",
      "  Batch: 101\n",
      "  Batch: 102\n",
      "  Batch: 103\n",
      "  Batch: 104\n",
      "  Batch: 105\n",
      "  Batch: 106\n",
      "  Batch: 107\n",
      "  Batch: 108\n",
      "  Batch: 109\n",
      "  Batch: 110\n",
      "  Batch: 111\n",
      "  Batch: 112\n",
      "  Batch: 113\n",
      "  Batch: 114\n",
      "  Batch: 115\n",
      "  Batch: 116\n",
      "  Batch: 117\n",
      "  Batch: 118\n",
      "  Batch: 119\n",
      "  Batch: 120\n",
      "  Batch: 121\n",
      "  Batch: 122\n",
      "  Batch: 123\n",
      "  Batch: 124\n",
      "  Batch: 125\n",
      "  Batch: 126\n",
      "  Batch: 127\n",
      "  Batch: 128\n",
      "  Batch: 129\n",
      "  Batch: 130\n",
      "  Batch: 131\n",
      "  Batch: 132\n",
      "  Batch: 133\n",
      "  Batch: 134\n",
      "  Batch: 135\n",
      "  Batch: 136\n",
      "  Batch: 137\n",
      "  Batch: 138\n",
      "  Batch: 139\n",
      "  Batch: 140\n",
      "  Batch: 141\n",
      "  Batch: 142\n",
      "  Batch: 143\n",
      "  Batch: 144\n",
      "  Batch: 145\n",
      "  Batch: 146\n",
      "  Batch: 147\n",
      "  Batch: 148\n",
      "  Batch: 149\n",
      "  Batch: 150\n",
      "  Batch: 151\n",
      "  Batch: 152\n",
      "  Batch: 153\n",
      "  Batch: 154\n",
      "  Batch: 155\n",
      "  Batch: 156\n",
      "  Batch: 157\n",
      "  Batch: 158\n",
      "  Batch: 159\n",
      "  Batch: 160\n",
      "  Batch: 161\n",
      "  Batch: 162\n",
      "  Batch: 163\n",
      "  Batch: 164\n",
      "  Batch: 165\n",
      "  Batch: 166\n",
      "  Batch: 167\n",
      "  Batch: 168\n",
      "  Batch: 169\n",
      "  Batch: 170\n",
      "  Batch: 171\n",
      "  Batch: 172\n",
      "  Batch: 173\n",
      "  Batch: 174\n",
      "  Batch: 175\n",
      "  Batch: 176\n",
      "  Batch: 177\n",
      "  Batch: 178\n",
      "  Batch: 179\n",
      "  Batch: 180\n",
      "  Batch: 181\n",
      "  Batch: 182\n",
      "  Batch: 183\n",
      "  Batch: 184\n",
      "  Batch: 185\n",
      "  Batch: 186\n",
      "  Batch: 187\n",
      "  Batch: 188\n",
      "  Batch: 189\n",
      "  Batch: 190\n",
      "  Batch: 191\n",
      "  Batch: 192\n",
      "  Batch: 193\n",
      "  Batch: 194\n",
      "  Batch: 195\n",
      "  Batch: 196\n",
      "  Batch: 197\n",
      "  Batch: 198\n",
      "  Batch: 199\n",
      "  Batch: 200\n",
      "  Batch: 201\n",
      "  Batch: 202\n",
      "  Batch: 203\n",
      "  Batch: 204\n",
      "  Batch: 205\n",
      "  Batch: 206\n",
      "  Batch: 207\n",
      "  Batch: 208\n",
      "  Batch: 209\n",
      "  Batch: 210\n",
      "  Batch: 211\n",
      "  Batch: 212\n",
      "  Batch: 213\n",
      "  Batch: 214\n",
      "  Batch: 215\n",
      "  Batch: 216\n",
      "  Batch: 217\n",
      "  Batch: 218\n",
      "  Batch: 219\n",
      "  Batch: 220\n",
      "  Batch: 221\n",
      "  Batch: 222\n",
      "  Batch: 223\n",
      "  Batch: 224\n",
      "  Batch: 225\n",
      "  Batch: 226\n",
      "  Batch: 227\n",
      "  Batch: 228\n",
      "  Batch: 229\n",
      "  Batch: 230\n",
      "  Batch: 231\n",
      "  Batch: 232\n",
      "  Batch: 233\n",
      "  Batch: 234\n",
      "  Batch: 235\n",
      "  Batch: 236\n",
      "  Batch: 237\n",
      "  Batch: 238\n",
      "  Batch: 239\n",
      "  Batch: 240\n",
      "  Batch: 241\n",
      "  Batch: 242\n",
      "  Batch: 243\n",
      "  Batch: 244\n",
      "  Batch: 245\n",
      "  Batch: 246\n",
      "  Batch: 247\n",
      "  Batch: 248\n",
      "  Batch: 249\n",
      "  Batch: 250\n",
      "  Batch: 251\n",
      "  Batch: 252\n",
      "  Batch: 253\n",
      "  Batch: 254\n",
      "  Batch: 255\n",
      "  Batch: 256\n",
      "  Batch: 257\n",
      "  Batch: 258\n",
      "  Batch: 259\n",
      "  Batch: 260\n",
      "  Batch: 261\n",
      "  Batch: 262\n",
      "  Batch: 263\n",
      "  Batch: 264\n",
      "  Batch: 265\n",
      "  Batch: 266\n",
      "  Batch: 267\n",
      "  Batch: 268\n",
      "  Batch: 269\n",
      "  Batch: 270\n",
      "  Batch: 271\n",
      "  Batch: 272\n",
      "  Batch: 273\n",
      "  Batch: 274\n",
      "  Batch: 275\n",
      "  Batch: 276\n",
      "  Batch: 277\n",
      "  Batch: 278\n",
      "  Batch: 279\n",
      "  Batch: 280\n",
      "  Batch: 281\n",
      "  Batch: 282\n",
      "  Batch: 283\n",
      "  Batch: 284\n",
      "  Batch: 285\n",
      "  Batch: 286\n",
      "  Batch: 287\n",
      "  Batch: 288\n",
      "  Batch: 289\n",
      "  Batch: 290\n",
      "  Batch: 291\n",
      "  Batch: 292\n",
      "  Batch: 293\n",
      "  Batch: 294\n",
      "  Batch: 295\n",
      "  Batch: 296\n",
      "  Batch: 297\n",
      "  Batch: 298\n",
      "  Batch: 299\n",
      "  Batch: 300\n",
      "  Batch: 301\n",
      "  Batch: 302\n",
      "  Batch: 303\n",
      "  Batch: 304\n",
      "  Batch: 305\n",
      "  Batch: 306\n",
      "  Batch: 307\n",
      "  Batch: 308\n",
      "  Batch: 309\n",
      "  Batch: 310\n",
      "  Batch: 311\n",
      "  Batch: 312\n",
      "  Batch: 313\n",
      "  Batch: 314\n",
      "  Batch: 315\n",
      "  Batch: 316\n",
      "  Batch: 317\n",
      "  Batch: 318\n",
      "  Batch: 319\n",
      "  Batch: 320\n",
      "  Batch: 321\n",
      "  Batch: 322\n",
      "  Batch: 323\n",
      "  Batch: 324\n",
      "  Batch: 325\n",
      "  Batch: 326\n",
      "  Batch: 327\n",
      "  Batch: 328\n",
      "  Batch: 329\n",
      "  Batch: 330\n",
      "  Batch: 331\n",
      "  Batch: 332\n",
      "  Batch: 333\n",
      "  Batch: 334\n",
      "  Batch: 335\n",
      "  Batch: 336\n",
      "  Batch: 337\n",
      "  Batch: 338\n",
      "  Batch: 339\n",
      "  Batch: 340\n",
      "  Batch: 341\n",
      "  Batch: 342\n",
      "  Batch: 343\n",
      "  Batch: 344\n",
      "  Batch: 345\n",
      "  Batch: 346\n",
      "  Batch: 347\n",
      "  Batch: 348\n",
      "  Batch: 349\n",
      "  Batch: 350\n",
      "  Batch: 351\n",
      "  Batch: 352\n",
      "  Batch: 353\n",
      "  Batch: 354\n",
      "  Batch: 355\n",
      "  Batch: 356\n",
      "  Batch: 357\n",
      "  Batch: 358\n",
      "  Batch: 359\n",
      "  Batch: 360\n",
      "  Batch: 361\n",
      "  Batch: 362\n",
      "  Batch: 363\n",
      "  Batch: 364\n",
      "  Batch: 365\n",
      "  Batch: 366\n",
      "  Batch: 367\n",
      "  Batch: 368\n",
      "  Batch: 369\n",
      "  Batch: 370\n",
      "  Batch: 371\n",
      "  Batch: 372\n",
      "  Batch: 373\n",
      "  Batch: 374\n",
      "  Batch: 375\n",
      "  Batch: 376\n",
      "  Batch: 377\n",
      "  Batch: 378\n",
      "  Batch: 379\n",
      "  Batch: 380\n",
      "  Batch: 381\n",
      "  Batch: 382\n",
      "  Batch: 383\n",
      "  Batch: 384\n",
      "  Batch: 385\n",
      "  Batch: 386\n",
      "  Batch: 387\n",
      "  Batch: 388\n",
      "\n",
      "Creating test array:\n",
      "  Batch: 0\n",
      "  Batch: 1\n",
      "  Batch: 2\n",
      "  Batch: 3\n",
      "  Batch: 4\n",
      "  Batch: 5\n",
      "  Batch: 6\n",
      "  Batch: 7\n",
      "  Batch: 8\n",
      "  Batch: 9\n",
      "  Batch: 10\n",
      "  Batch: 11\n",
      "  Batch: 12\n",
      "  Batch: 13\n",
      "  Batch: 14\n",
      "  Batch: 15\n",
      "  Batch: 16\n",
      "  Batch: 17\n",
      "  Batch: 18\n",
      "  Batch: 19\n",
      "  Batch: 20\n",
      "  Batch: 21\n",
      "  Batch: 22\n",
      "  Batch: 23\n",
      "  Batch: 24\n",
      "  Batch: 25\n",
      "  Batch: 26\n",
      "  Batch: 27\n",
      "  Batch: 28\n",
      "  Batch: 29\n",
      "  Batch: 30\n",
      "  Batch: 31\n",
      "  Batch: 32\n",
      "  Batch: 33\n",
      "  Batch: 34\n",
      "  Batch: 35\n",
      "  Batch: 36\n",
      "  Batch: 37\n",
      "  Batch: 38\n",
      "  Batch: 39\n",
      "  Batch: 40\n",
      "  Batch: 41\n",
      "  Batch: 42\n",
      "  Batch: 43\n",
      "  Batch: 44\n",
      "  Batch: 45\n",
      "  Batch: 46\n",
      "  Batch: 47\n",
      "  Batch: 48\n",
      "  Batch: 49\n",
      "  Batch: 50\n",
      "  Batch: 51\n",
      "  Batch: 52\n",
      "  Batch: 53\n",
      "  Batch: 54\n",
      "  Batch: 55\n",
      "  Batch: 56\n",
      "  Batch: 57\n",
      "  Batch: 58\n",
      "  Batch: 59\n",
      "  Batch: 60\n",
      "  Batch: 61\n",
      "  Batch: 62\n",
      "  Batch: 63\n",
      "  Batch: 64\n",
      "  Batch: 65\n",
      "  Batch: 66\n",
      "  Batch: 67\n",
      "  Batch: 68\n",
      "  Batch: 69\n",
      "  Batch: 70\n",
      "  Batch: 71\n",
      "  Batch: 72\n",
      "  Batch: 73\n",
      "  Batch: 74\n",
      "  Batch: 75\n",
      "  Batch: 76\n",
      "  Batch: 77\n",
      "  Batch: 78\n",
      "  Batch: 79\n",
      "  Batch: 80\n",
      "  Batch: 81\n",
      "  Batch: 82\n",
      "  Batch: 83\n",
      "  Batch: 84\n",
      "  Batch: 85\n",
      "  Batch: 86\n",
      "  Batch: 87\n",
      "  Batch: 88\n",
      "  Batch: 89\n",
      "  Batch: 90\n",
      "  Batch: 91\n",
      "  Batch: 92\n",
      "  Batch: 93\n",
      "  Batch: 94\n",
      "  Batch: 95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "print(\"Creating training array: \")\n",
    "for i in range(train_max_batches):\n",
    "    print(f\"  Batch: {i}\")\n",
    "    batch = train_datagen.next()\n",
    "    x_train.extend(batch[0])\n",
    "    y_train.extend(batch[1])\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "print(\"\\nCreating test array:\")\n",
    "for i in range(test_max_batches):\n",
    "    print(f\"  Batch: {i}\")\n",
    "    batch = test_datagen.next()\n",
    "    x_test.extend(batch[0])\n",
    "    y_test.extend(batch[1])\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oS9DHNV3jU9H"
   },
   "source": [
    "## Save train and test numpy dataset array to disk\n",
    "\n",
    "*   List item\n",
    "*   List item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3OG8eceqjU9H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import save\n",
    "\n",
    "# ds_arrays_path = '/content/drive/MyDrive/TG/dataset_arrays'\n",
    "ds_arrays_path = '/home/beduinigor/dataset_arrays'\n",
    "\n",
    "save(os.path.join(ds_arrays_path, 'x_train_data.npy'), x_train)\n",
    "save(os.path.join(ds_arrays_path, 'y_train_data.npy'), y_train)\n",
    "\n",
    "save(os.path.join(ds_arrays_path, 'x_test_data.npy'), x_test)\n",
    "save(os.path.join(ds_arrays_path, 'y_test_data.npy'), y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reju83mkkeEV"
   },
   "source": [
    "## Load train and test numpy dataset array to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 99587,
     "status": "ok",
     "timestamp": 1611718312517,
     "user": {
      "displayName": "Igor Beduin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicrLX4A8XIurRrs5tUPFTs_mZ6LzjddQeBQkKTng=s64",
      "userId": "12542818864135131012"
     },
     "user_tz": 180
    },
    "id": "Luan4zu9jU9H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import load\n",
    "\n",
    "# ds_arrays_path = '/content/drive/MyDrive/TG/dataset_arrays'\n",
    "ds_arrays_path = '/home/beduinigor/dataset_arrays'\n",
    "\n",
    "x_train = load(os.path.join(ds_arrays_path,'x_train_data.npy'))\n",
    "y_train = load(os.path.join(ds_arrays_path,'y_train_data.npy'))\n",
    "x_test = load(os.path.join(ds_arrays_path,'x_test_data.npy'))\n",
    "y_test = load(os.path.join(ds_arrays_path,'y_test_data.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98124,
     "status": "ok",
     "timestamp": 1611718312520,
     "user": {
      "displayName": "Igor Beduin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicrLX4A8XIurRrs5tUPFTs_mZ6LzjddQeBQkKTng=s64",
      "userId": "12542818864135131012"
     },
     "user_tz": 180
    },
    "id": "ojbQgsUStRbs",
    "outputId": "cc313ccd-2273-4389-ed44-52878d068647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "----------\n",
      "(12448, 240, 240, 1)\n",
      "(12448, 3)\n",
      "\n",
      "TEST\n",
      "----------\n",
      "(3072, 240, 240, 1)\n",
      "(3072, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN\\n----------\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"\\nTEST\\n----------\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1611718336178,
     "user": {
      "displayName": "Igor Beduin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicrLX4A8XIurRrs5tUPFTs_mZ6LzjddQeBQkKTng=s64",
      "userId": "12542818864135131012"
     },
     "user_tz": 180
    },
    "id": "5dIrlFgZtRbt"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tb_log_dir = \"/Users/igorbeduin/Google Drive (beduinigor@gmail.com)/TG/tensorboard\"\n",
    "tb_log_dir = '/home/beduinigor/training/tensorboard'\n",
    "\n",
    "checkpoint_path = \"/home/beduinigor/tg2_COVIDNet/checkpoints/weights.epoch:{epoch:02d}-acc:{accuracy:.2f}.h5\"\n",
    "checkpoint_epochs = 7000\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor=\"val_loss\",\n",
    "                    min_delta=0,\n",
    "                    patience=20,\n",
    "                    verbose=1,\n",
    "                    mode=\"auto\",\n",
    "                    baseline=None,\n",
    "                    restore_best_weights=True)\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "                    log_dir=tb_log_dir,\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=False,\n",
    "                    update_freq=\"epoch\",\n",
    "                    profile_batch=100000000000,\n",
    "                    embeddings_freq=0,\n",
    "                    embeddings_metadata=None)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                    checkpoint_path,\n",
    "                    monitor=\"val_loss\",\n",
    "                    verbose=1,\n",
    "                    save_best_only=False,\n",
    "                    save_weights_only=False,\n",
    "                    mode=\"auto\",\n",
    "                    save_freq=checkpoint_epochs,\n",
    "                    options=None)\n",
    "\n",
    "callbacks = [tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okAEcA3SMFT4"
   },
   "source": [
    "### Searching on resnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "executionInfo": {
     "elapsed": 16946,
     "status": "error",
     "timestamp": 1611718442414,
     "user": {
      "displayName": "Igor Beduin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicrLX4A8XIurRrs5tUPFTs_mZ6LzjddQeBQkKTng=s64",
      "userId": "12542818864135131012"
     },
     "user_tz": 180
    },
    "id": "CD3LqeLktRbu",
    "outputId": "fc504b7a-1d48-41a2-89a8-910c93003b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project /home/beduinigor/training/models/auto_model/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from /home/beduinigor/training/models/auto_model/tuner0.json\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "image_block_1/r...|True              |True              \n",
      "image_block_1/r...|resnet152         |resnet152         \n",
      "image_block_1/r...|False             |False             \n",
      "classification_...|flatten           |global_avg        \n",
      "classification_...|0.5               |0.5               \n",
      "optimizer         |adam_weight_decay |adam_weight_decay \n",
      "learning_rate     |1e-05             |1e-05             \n",
      "image_block_1/r...|False             |False             \n",
      "\n",
      "Epoch 1/100\n",
      "312/312 [==============================] - 54s 173ms/step - loss: 2.3357 - accuracy: 0.3677 - val_loss: 0.9202 - val_accuracy: 0.5491\n",
      "Epoch 2/100\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 1.4857 - accuracy: 0.4902 - val_loss: 0.8795 - val_accuracy: 0.5524\n",
      "Epoch 3/100\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 1.4503 - accuracy: 0.5025 - val_loss: 0.8483 - val_accuracy: 0.5629\n",
      "Epoch 4/100\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 1.4113 - accuracy: 0.5095 - val_loss: 0.7996 - val_accuracy: 0.6615\n",
      "Epoch 5/100\n",
      "188/312 [=================>............] - ETA: 15s - loss: 1.3913 - accuracy: 0.5199"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import autokeras as ak\n",
    "\n",
    "# model_path = \"/content/drive/MyDrive/TG/training/models\"\n",
    "model_path = \"/home/beduinigor/training/models\"\n",
    "\n",
    "input_node = ak.ImageInput()\n",
    "output_node = ak.ImageBlock(\n",
    "    # Only search ResNet architectures.\n",
    "    block_type=\"resnet\",\n",
    "    # Normalize the dataset.\n",
    "    normalize=False,\n",
    "    # Do not do data augmentation.\n",
    "    augment=False,\n",
    "    )(input_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "\n",
    "model = ak.AutoModel(\n",
    "    inputs=input_node, \n",
    "    outputs=output_node,\n",
    "    directory=model_path,\n",
    "    overwrite=False,\n",
    "    max_trials=100)\n",
    "\n",
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks, \n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    verbose=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ptu_yzqMFT6"
   },
   "source": [
    "### Searching on every classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 27220,
     "status": "error",
     "timestamp": 1611718366370,
     "user": {
      "displayName": "Igor Beduin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GicrLX4A8XIurRrs5tUPFTs_mZ6LzjddQeBQkKTng=s64",
      "userId": "12542818864135131012"
     },
     "user_tz": 180
    },
    "id": "PJAWeU3lMFT9",
    "outputId": "5cbf4793-89cf-4aaf-9306-ceaa4c34e77c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "image_block_1/b...|vanilla           |?                 \n",
      "image_block_1/n...|True              |?                 \n",
      "image_block_1/a...|False             |?                 \n",
      "image_block_1/c...|3                 |?                 \n",
      "image_block_1/c...|1                 |?                 \n",
      "image_block_1/c...|2                 |?                 \n",
      "image_block_1/c...|True              |?                 \n",
      "image_block_1/c...|False             |?                 \n",
      "image_block_1/c...|0.25              |?                 \n",
      "image_block_1/c...|32                |?                 \n",
      "image_block_1/c...|64                |?                 \n",
      "classification_...|flatten           |?                 \n",
      "classification_...|0.5               |?                 \n",
      "optimizer         |adam              |?                 \n",
      "learning_rate     |0.001             |?                 \n",
      "\n",
      "Epoch 1/100\n",
      "312/312 [==============================] - 26s 82ms/step - loss: 0.6874 - accuracy: 0.7717 - val_loss: 0.5264 - val_accuracy: 0.7950\n",
      "Epoch 2/100\n",
      "312/312 [==============================] - 26s 83ms/step - loss: 0.4624 - accuracy: 0.8224 - val_loss: 0.4798 - val_accuracy: 0.8068\n",
      "Epoch 3/100\n",
      "312/312 [==============================] - 26s 83ms/step - loss: 0.3440 - accuracy: 0.8698 - val_loss: 0.5484 - val_accuracy: 0.7979\n",
      "Epoch 4/100\n",
      "312/312 [==============================] - 26s 82ms/step - loss: 0.2185 - accuracy: 0.9166 - val_loss: 0.6583 - val_accuracy: 0.8056\n",
      "Epoch 5/100\n",
      "312/312 [==============================] - 26s 83ms/step - loss: 0.1498 - accuracy: 0.9427 - val_loss: 0.8739 - val_accuracy: 0.8011\n",
      "Epoch 6/100\n",
      "237/312 [=====================>........] - ETA: 4s - loss: 0.1252 - accuracy: 0.9532"
     ]
    }
   ],
   "source": [
    "import autokeras as ak\n",
    "import tensorflow as tf\n",
    "\n",
    "# model_path = \"/content/drive/MyDrive/TG/training/models\"\n",
    "model_path = \"/home/beduinigor/training/models\"\n",
    "\n",
    "model = ak.ImageClassifier(\n",
    "    num_classes=3,\n",
    "    overwrite=False,\n",
    "    directory=model_path,\n",
    "    max_trials=100)\n",
    "\n",
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks, \n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cozsE4g2tRbv"
   },
   "outputs": [],
   "source": [
    "trained_model = model.export_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zx1c_rXUtRbw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "finalmodel_dir = '/home/beduinigor/training/models'\n",
    "\n",
    "trained_model.save(os.path.join(finalmodel_dir, \"final_model.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-gim3JptRbw"
   },
   "source": [
    "## LEITURA DE PESOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3NP1Zu-tRbw"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "#checkpoints_path = \"/home/beduinigor/tg2_COVIDNet/checkpoints\"\n",
    "auto_model_path = \"/media/training/covidnet/auto_model\"\n",
    "best_model_path = \"/media/training/covidnet/checkpoints/final_model.h5\"\n",
    "\n",
    "loaded_model = load_model(best_model_path)\n",
    "print(loaded_model.evaluate(x_test, y_test, batch_size=batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2l8Hy5btRb9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hcfo9r7atRbN",
    "lPeVZeGCtRbT",
    "DzDtxm8KtRbU",
    "0UWyaQaDtRbV",
    "Vm91D7QJtRbX",
    "RXUO0ypjtRbZ",
    "64NCOWBhtRba",
    "K71Np5GPtRbb",
    "Cf1NECVRtRbk"
   ],
   "name": "data_generator.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "covidnet_igor",
   "language": "python",
   "name": "covidnet_igor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
